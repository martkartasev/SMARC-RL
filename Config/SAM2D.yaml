# Generally, higher learning rate with larger batch size and more epochs seems to do a good job.
# But this can take too long to train, mainly due to number of epochs.
# Therefore, lower learning rate, smaller batch size and less epochs 
# seems to work fine as a training time upgrade without losing too in terms of behavior.

behaviors:
  2DAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 10000
      buffer_size: 100000
      learning_rate: 0.0005 # Def: 0.0003 (1e-5 to 1e-3)
      beta: 0.05 # (0.05 - 0.0005) Default 0.005 . Entropy regularization. Randomness / Exploration.
      epsilon: 0.25 # Max policy step (0.1 - 0.3)
      lambd: 0.9 # (0.9 - 0.95) Regularisation for GAE. How much the agent relies on current value estimate.
      num_epoch: 3
      shared_critic: false
      learning_rate_schedule: linear
      beta_schedule: linear
      epsilon_schedule: linear
    network_settings:
      normalize: false
      hidden_units: 256
      num_layers: 3
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
    keep_checkpoints: 5
    even_checkpoints: false
    checkpoint_interval: 500000
    max_steps: 100000000
    time_horizon: 2500
    summary_freq: 100000
    #init_path: None   # Initial policy to use for initialization
    threaded: false
    
engine_settings:
  width: 84
  height: 84
  time_scale: 100
  target_frame_rate: -1
  no_graphics: true
  
# mlagents-learn config/SAMSimple.yaml --run-id retrain_test_01 --env Build/SAM-RL --num-envs 6 --base-port 8000